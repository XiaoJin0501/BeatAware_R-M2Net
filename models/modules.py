import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

# ==========================================
# Part 1: TFiLM (Temporal Feature-wise Linear Modulation)
# Adapted for 1D Time-Series (Radar/ECG)
# ==========================================

class TFiLMLayer(nn.Module):
    """
    TFiLM Layer: Applies dynamic scale and shift to features.
    
    Equation:
        y = x * gamma + beta
    
    Args:
        - x: Input feature map [Batch, Channels, Length]
        - gamma: Scale parameter [Batch, Channels, Length] (generated by Condition Network)
        - beta: Shift parameter [Batch, Channels, Length] (generated by Condition Network)
    """
    def __init__(self):
        super(TFiLMLayer, self).__init__()

    def forward(self, x, gamma, beta):
        # 确保维度匹配 (Explicit broadcasting check)
        # 如果 gamma/beta 是 [B, C, 1]，它们会自动广播到 [B, C, L]
        return x * gamma + beta

class TFiLMGenerator(nn.Module):
    """
    Lightweight Generator to create gamma/beta from the Beat-Probability Mask.
    
    Input: Mask [Batch, 1, Length]
    Output: gamma, beta [Batch, Target_Channels, Length]
    """
    def __init__(self, input_dim=1, target_channels=64, kernel_size=3):
        super(TFiLMGenerator, self).__init__()
        padding = kernel_size // 2
        
        # 使用卷积从 Mask 生成调制参数，保持时序对齐
        self.conv_gamma = nn.Conv1d(input_dim, target_channels, kernel_size, padding=padding)
        self.conv_beta = nn.Conv1d(input_dim, target_channels, kernel_size, padding=padding)
        
        self.reset_parameters()

    def reset_parameters(self):
        # 初始化：Gamma 初始为 1 (不缩放)，Beta 初始为 0 (不偏移)
        # 这样初始状态下 TFiLM 不会破坏原始特征，这是稳定训练的关键！
        nn.init.constant_(self.conv_gamma.weight, 0.0)
        nn.init.constant_(self.conv_gamma.bias, 1.0)
        
        nn.init.constant_(self.conv_beta.weight, 0.0)
        nn.init.constant_(self.conv_beta.bias, 0.0)

    def forward(self, mask):
        gamma = self.conv_gamma(mask)
        beta = self.conv_beta(mask)
        return gamma, beta


# ==========================================
# Part 2: Conformer Components
# Ref: https://arxiv.org/abs/2005.08100
# Optimized for fixed-length signals (removed dynamic masking)
# ==========================================

class Swish(nn.Module):
    """Swish activation function: x * sigmoid(x)"""
    def forward(self, x):
        return x * torch.sigmoid(x)

class FeedForwardModule(nn.Module):
    """
    Conformer Feed Forward Module
    Structure: Norm -> Linear -> Swish -> Dropout -> Linear -> Dropout
    """
    def __init__(self, dim, expansion_factor=4, dropout=0.1):
        super(FeedForwardModule, self).__init__()
        self.layer_norm = nn.LayerNorm(dim)
        self.linear1 = nn.Linear(dim, dim * expansion_factor)
        self.activation = Swish()
        self.dropout1 = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim * expansion_factor, dim)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        # x: [Batch, Length, Dim]
        out = self.layer_norm(x)
        out = self.linear1(out)
        out = self.activation(out)
        out = self.dropout1(out)
        out = self.linear2(out)
        out = self.dropout2(out)
        return out

class ConformerConvModule(nn.Module):
    """
    Conformer Convolution Module
    Structure: LayerNorm -> Pointwise Conv -> Gated Linear Unit (GLU) -> Depthwise Conv -> Batch Norm -> Swish -> Pointwise Conv
    """
    def __init__(self, in_channels, kernel_size=31, expansion_factor=2, dropout=0.1):
        super(ConformerConvModule, self).__init__()
        
        assert (kernel_size - 1) % 2 == 0, "Kernel size must be odd for 'SAME' padding"
        padding = (kernel_size - 1) // 2
        
        self.layer_norm = nn.LayerNorm(in_channels)
        
        # Pointwise Conv (expansion)
        self.pointwise_conv1 = nn.Conv1d(in_channels, in_channels * expansion_factor, kernel_size=1, stride=1, padding=0, bias=True)
        self.glu = nn.GLU(dim=1) # GLU halves the channels
        
        # Depthwise Conv (Spatial mixing)
        self.depthwise_conv = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, groups=in_channels, bias=True)
        
        self.batch_norm = nn.BatchNorm1d(in_channels)
        self.activation = Swish()
        
        # Pointwise Conv (Projection)
        self.pointwise_conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=True)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Input x: [Batch, Length, Dim]
        # Conformer internally works with [Batch, Dim, Length] for Convs
        
        x = self.layer_norm(x)
        x = x.transpose(1, 2) # [B, L, D] -> [B, D, L]
        
        x = self.pointwise_conv1(x)
        x = self.glu(x)
        x = self.depthwise_conv(x)
        x = self.batch_norm(x)
        x = self.activation(x)
        x = self.pointwise_conv2(x)
        x = self.dropout(x)
        
        x = x.transpose(1, 2) # [B, D, L] -> [B, L, D]
        return x

class MultiHeadSelfAttentionModule(nn.Module):
    """
    Wrapper for MHSA with Residual connection
    """
    def __init__(self, dim, num_heads=4, dropout=0.1):
        super(MultiHeadSelfAttentionModule, self).__init__()
        self.layer_norm = nn.LayerNorm(dim)
        self.mha = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: [Batch, Length, Dim]
        out = self.layer_norm(x)
        out, _ = self.mha(out, out, out) # Self-Attention
        out = self.dropout(out)
        return out

class ConformerFusionBlock(nn.Module):
    """
    [Core Module for Beat-Aware R-M2Net]
    Standard Conformer Block (Macaron style) adapted for 1D Signal Fusion.
    
    Structure:
      x = x + 0.5 * FFN(x)
      x = x + MHSA(x)
      x = x + Conv(x)
      x = x + 0.5 * FFN(x)
      y = LayerNorm(x)
    """
    def __init__(self, dim, num_heads=4, ffn_expansion=4, conv_kernel_size=31, dropout=0.1):
        super(ConformerFusionBlock, self).__init__()
        
        self.ffn1 = FeedForwardModule(dim, ffn_expansion, dropout)
        self.mhsa = MultiHeadSelfAttentionModule(dim, num_heads, dropout)
        self.conv = ConformerConvModule(dim, conv_kernel_size, expansion_factor=2, dropout=dropout)
        self.ffn2 = FeedForwardModule(dim, ffn_expansion, dropout)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        # Input x: [Batch, Channels, Length] -> 需要转为 [Batch, Length, Channels]
        x = x.transpose(1, 2) 
        
        # Macaron Style Feed Forward (First half)
        x = x + 0.5 * self.ffn1(x)
        
        # Multi-Head Self-Attention
        x = x + self.mhsa(x)
        
        # Convolution Module
        x = x + self.conv(x)
        
        # Macaron Style Feed Forward (Second half)
        x = x + 0.5 * self.ffn2(x)
        
        # Final Norm
        x = self.norm(x)
        
        # Output: [Batch, Length, Channels] -> 转回 [Batch, Channels, Length]
        x = x.transpose(1, 2)
        return x

# ==========================================
# Part 3: Basic Conv Block (Standard)
# ==========================================
class Conv1DBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1):
        super(Conv1DBlock, self).__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation=dilation)
        self.bn = nn.BatchNorm1d(out_channels)
        self.act = nn.PReLU() # PReLU is good for signal reconstruction

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))